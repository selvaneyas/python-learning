{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Day18: k-Nearest Neighbors (k-NN) â€“ Classification & Regression\n",
    "\n",
    "k-NN is a powerful, yet simple algorithm used for **both classification and regression**. It makes predictions based on the **majority vote** of k-nearest neighbors in the feature space.\n",
    "\n",
    "## ğŸ”¹ **Key Features:**\n",
    "âœ… Works well with both **classification & regression**.\n",
    "\n",
    "âœ… Uses **distance measures** like Euclidean, Manhattan, etc.\n",
    "\n",
    "âœ… **Decision boundary visualization** helps understand how k-NN classifies data.\n",
    "\n",
    "## ğŸ›  **Implementation Highlights:**\n",
    "- ğŸ”¸ k-NN **Classification** â€“ Predicts classes based on **majority voting**.\n",
    "- ğŸ”¸ k-NN **Regression** â€“ Predicts values by averaging **k-nearest neighbors**.\n",
    "- ğŸ”¸ **Visualization of Decision Boundaries & Predictions** included.\n",
    "\n",
    "## ğŸ“Š **Results:**\n",
    "- âœ”ï¸ **Accuracy (Classification):** 91%\n",
    "- âœ”ï¸ **MSE (Regression):** 0.62, **RÂ² Score:** 0.75\n",
    "\n",
    "## ğŸ” **Key Insights:**\n",
    "- The choice of **k** affects model performance.\n",
    "- **Distance metric (Euclidean/Manhattan)** plays a crucial role.\n",
    "- **k-NN is simple yet effective** for many ML applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“„ knn_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“„ knn_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
