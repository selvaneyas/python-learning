{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 Day18: k-Nearest Neighbors (k-NN) – Classification & Regression\n",
    "\n",
    "k-NN is a powerful, yet simple algorithm used for **both classification and regression**. It makes predictions based on the **majority vote** of k-nearest neighbors in the feature space.\n",
    "\n",
    "## 🔹 **Key Features:**\n",
    "✅ Works well with both **classification & regression**.\n",
    "\n",
    "✅ Uses **distance measures** like Euclidean, Manhattan, etc.\n",
    "\n",
    "✅ **Decision boundary visualization** helps understand how k-NN classifies data.\n",
    "\n",
    "## 🛠 **Implementation Highlights:**\n",
    "- 🔸 k-NN **Classification** – Predicts classes based on **majority voting**.\n",
    "- 🔸 k-NN **Regression** – Predicts values by averaging **k-nearest neighbors**.\n",
    "- 🔸 **Visualization of Decision Boundaries & Predictions** included.\n",
    "\n",
    "## 📊 **Results:**\n",
    "- ✔️ **Accuracy (Classification):** 91%\n",
    "- ✔️ **MSE (Regression):** 0.62, **R² Score:** 0.75\n",
    "\n",
    "## 🔍 **Key Insights:**\n",
    "- The choice of **k** affects model performance.\n",
    "- **Distance metric (Euclidean/Manhattan)** plays a crucial role.\n",
    "- **k-NN is simple yet effective** for many ML applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📄 knn_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📄 knn_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
