{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ’»ğŸ¤– ğƒğšğ² ğŸğŸ•: ğ„ğ§ğ¬ğğ¦ğ›ğ¥ğ ğ‹ğğšğ«ğ§ğ¢ğ§ğ  â€“ ğ’ğ­ğšğœğ¤ğ¢ğ§ğ , ğğ¨ğ¨ğ¬ğ­ğ¢ğ§ğ  (ğ€ğğšğğ¨ğ¨ğ¬ğ­, ğ†ğ«ğšğğ¢ğğ§ğ­ ğğ¨ğ¨ğ¬ğ­ğ¢ğ§ğ , ğ—ğ†ğğ¨ğ¨ğ¬ğ­) | ğŸ‘ğŸ-ğƒğšğ² ğŒğ‹ ğ‚ğ¡ğšğ¥ğ¥ğğ§ğ ğ \n",
    "\n",
    "\n",
    "\n",
    "In the world of Machine Learning, Ensemble Learning is a game-changer! By combining multiple models, we can build more robust, accurate, and generalizable predictions.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¥ ğŠğğ² ğ„ğ§ğ¬ğğ¦ğ›ğ¥ğ ğ“ğğœğ¡ğ§ğ¢ğªğ®ğğ¬:\n",
    "\n",
    "\n",
    " ğŸ”¹ Stacking â€“ Combines multiple models and uses another model (meta-learner) to make the final prediction.\n",
    "\n",
    " ğŸ”¹ Boosting â€“ Focuses on correcting errors by training models sequentially. Popular boosting algorithms include:\n",
    "\n",
    "AdaBoost (Adaptive Boosting) â€“ Assigns higher weights to misclassified samples.\n",
    "\n",
    "Gradient Boosting â€“ Minimizes residual errors in a stepwise manner.\n",
    "\n",
    "XGBoost â€“ An optimized version of Gradient Boosting, fast and powerful!\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ’¡ ğ–ğ¡ğ² ğ„ğ§ğ¬ğğ¦ğ›ğ¥ğ ğ‹ğğšğ«ğ§ğ¢ğ§ğ ?\n",
    "\n",
    " âœ… Increases model accuracy\n",
    "\n",
    " âœ… Reduces overfitting\n",
    "\n",
    " âœ… Works well with complex datasets\n",
    "\n",
    " âœ… Handles both regression & classification tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
