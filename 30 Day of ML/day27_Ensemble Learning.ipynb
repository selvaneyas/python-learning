{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 💻🤖 𝐃𝐚𝐲 𝟐𝟕: 𝐄𝐧𝐬𝐞𝐦𝐛𝐥𝐞 𝐋𝐞𝐚𝐫𝐧𝐢𝐧𝐠 – 𝐒𝐭𝐚𝐜𝐤𝐢𝐧𝐠, 𝐁𝐨𝐨𝐬𝐭𝐢𝐧𝐠 (𝐀𝐝𝐚𝐁𝐨𝐨𝐬𝐭, 𝐆𝐫𝐚𝐝𝐢𝐞𝐧𝐭 𝐁𝐨𝐨𝐬𝐭𝐢𝐧𝐠, 𝐗𝐆𝐁𝐨𝐨𝐬𝐭) | 𝟑𝟎-𝐃𝐚𝐲 𝐌𝐋 𝐂𝐡𝐚𝐥𝐥𝐞𝐧𝐠𝐞 \n",
    "\n",
    "\n",
    "\n",
    "In the world of Machine Learning, Ensemble Learning is a game-changer! By combining multiple models, we can build more robust, accurate, and generalizable predictions.\n",
    "\n",
    "\n",
    "\n",
    "## 🔥 𝐊𝐞𝐲 𝐄𝐧𝐬𝐞𝐦𝐛𝐥𝐞 𝐓𝐞𝐜𝐡𝐧𝐢𝐪𝐮𝐞𝐬:\n",
    "\n",
    "\n",
    " 🔹 Stacking – Combines multiple models and uses another model (meta-learner) to make the final prediction.\n",
    "\n",
    " 🔹 Boosting – Focuses on correcting errors by training models sequentially. Popular boosting algorithms include:\n",
    "\n",
    "AdaBoost (Adaptive Boosting) – Assigns higher weights to misclassified samples.\n",
    "\n",
    "Gradient Boosting – Minimizes residual errors in a stepwise manner.\n",
    "\n",
    "XGBoost – An optimized version of Gradient Boosting, fast and powerful!\n",
    "\n",
    "\n",
    "\n",
    "## 💡 𝐖𝐡𝐲 𝐄𝐧𝐬𝐞𝐦𝐛𝐥𝐞 𝐋𝐞𝐚𝐫𝐧𝐢𝐧𝐠?\n",
    "\n",
    " ✅ Increases model accuracy\n",
    "\n",
    " ✅ Reduces overfitting\n",
    "\n",
    " ✅ Works well with complex datasets\n",
    "\n",
    " ✅ Handles both regression & classification tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
