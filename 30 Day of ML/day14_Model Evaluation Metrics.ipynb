{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Š ğƒğšğ² ğŸğŸ’: ğŒğ¨ğğğ¥ ğ„ğ¯ğšğ¥ğ®ğšğ­ğ¢ğ¨ğ§ ğŒğğ­ğ«ğ¢ğœğ¬ â€“ ğŒğ€ğ„, ğŒğ’ğ„, ğ‘ğŒğ’ğ„, ğ€ğœğœğ®ğ«ğšğœğ², ğğ«ğğœğ¢ğ¬ğ¢ğ¨ğ§, ğ‘ğğœğšğ¥ğ¥ | ğŸ‘ğŸ-ğƒğšğ² ğŒğ‹ ğ‚ğ¡ğšğ¥ğ¥ğğ§ğ ğ\n",
    "\n",
    "\n",
    "\n",
    "Evaluating a Machine Learning model is crucial to measure how well it performs on unseen data. Different problems require different evaluation metrics!\n",
    "\n",
    "\n",
    "\n",
    "## âœ…ğ–ğ¡ğ² ğŒğ¨ğğğ¥ ğ„ğ¯ğšğ¥ğ®ğšğ­ğ¢ğ¨ğ§ ğ¢ğ¬ ğˆğ¦ğ©ğ¨ğ«ğ­ğšğ§ğ­?\n",
    "\n",
    "     ğŸ¯ Assess model's prediction quality\n",
    "\n",
    "     ğŸ“ˆ Compare multiple models\n",
    "\n",
    "     ğŸ” Improve model performance through tuning\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”‘ ğŠğğ² ğ„ğ¯ğšğ¥ğ®ğšğ­ğ¢ğ¨ğ§ ğŒğğ­ğ«ğ¢ğœğ¬:\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ”µ ğ‘­ğ’ğ’“ ğ‘¹ğ’†ğ’ˆğ’“ğ’†ğ’”ğ’”ğ’Šğ’ğ’ ğ‘·ğ’“ğ’ğ’ƒğ’ğ’†ğ’ğ’”:\n",
    "\n",
    "1ï¸âƒ£ ğ— ğ—”ğ—˜ (ğ— ğ—²ğ—®ğ—» ğ—”ğ—¯ğ˜€ğ—¼ğ—¹ğ˜‚ğ˜ğ—² ğ—˜ğ—¿ğ—¿ğ—¼ğ—¿) â€“ Average of absolute errors.\n",
    "\n",
    "     â¡ï¸ Measures average magnitude of errors.\n",
    "\n",
    "2ï¸âƒ£ ğ— ğ—¦ğ—˜ (ğ— ğ—²ğ—®ğ—» ğ—¦ğ—¾ğ˜‚ğ—®ğ—¿ğ—²ğ—± ğ—˜ğ—¿ğ—¿ğ—¼ğ—¿) â€“ Average of squared errors.\n",
    "\n",
    "     â¡ï¸ Penalizes larger errors more than smaller ones.\n",
    "\n",
    "3ï¸âƒ£ ğ—¥ğ— ğ—¦ğ—˜ (ğ—¥ğ—¼ğ—¼ğ˜ ğ— ğ—²ğ—®ğ—» ğ—¦ğ—¾ğ˜‚ğ—®ğ—¿ğ—²ğ—± ğ—˜ğ—¿ğ—¿ğ—¼ğ—¿) â€“ Square root of MSE.\n",
    "\n",
    "     â¡ï¸ Interpretable in same units as target variable.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸŸ¢ ğ‘­ğ’ğ’“ ğ‘ªğ’ğ’‚ğ’”ğ’”ğ’Šğ’‡ğ’Šğ’„ğ’‚ğ’•ğ’Šğ’ğ’ ğ‘·ğ’“ğ’ğ’ƒğ’ğ’†ğ’ğ’”:\n",
    "\n",
    "4ï¸âƒ£ ğ—”ğ—°ğ—°ğ˜‚ğ—¿ğ—®ğ—°ğ˜† â€“ Proportion of correctly classified samples.\n",
    "\n",
    "      â¡ï¸ Simple but can be misleading for imbalanced datasets.\n",
    "\n",
    "5ï¸âƒ£ ğ—£ğ—¿ğ—²ğ—°ğ—¶ğ˜€ğ—¶ğ—¼ğ—» â€“ True Positives / (True Positives + False Positives).\n",
    "\n",
    "      â¡ï¸ How many predicted positives are actually positive.\n",
    "\n",
    "6ï¸âƒ£ ğ—¥ğ—²ğ—°ğ—®ğ—¹ğ—¹ â€“ True Positives / (True Positives + False Negatives).\n",
    "\n",
    "      â¡ï¸ Ability to find all actual positives.\n",
    "\n",
    "\n",
    "\n",
    "## âœ… ğ‘°ğ’ğ’”ğ’Šğ’ˆğ’‰ğ’•ğ’”:\n",
    "\n",
    "      ğŸ“Š Use MAE, MSE, RMSE for regression to measure error magnitude.\n",
    "\n",
    "      ğŸ§  Accuracy is good for balanced datasets, but Precision & Recall are better for imbalanced ones (like fraud detection, medical diagnosis).\n",
    "\n",
    "      ğŸ” Always choose metric based on business problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“„ regression_metrics (MAE, MSE, RMSE, RÂ²)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression metrics Example\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example actual and predicted values\n",
    "y_true = np.array([3, -0.5, 2, 7])\n",
    "y_pred = np.array([2.5, 0.0, 2, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error [MAE]: 0.500\n",
      "Mean Squared Error [MSE]: 0.375\n",
      "Root Mean Squared Error [RMSE]: 0.612\n",
      "RÂ² Score: 0.949\n"
     ]
    }
   ],
   "source": [
    "print(f'Mean Absolute Error [MAE]: {mae:.3f}')\n",
    "print(f'Mean Squared Error [MSE]: {mse:.3f}')\n",
    "print(f'Root Mean Squared Error [RMSE]: {rmse:.3f}')\n",
    "print(f\"RÂ² Score: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“„ classification_metrics (Accuracy, Precision, Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
